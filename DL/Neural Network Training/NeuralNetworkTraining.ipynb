{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **신경망 학습**\n",
    "\n",
    "\n",
    "신경망 모델의 매개변수(가중치와 편향)를 데이터로부터 자동으로 학습하는 과정을 말합니다. 이 학습 과정은 주어진 입력 데이터와 해당 데이터에 대한 정답(타깃)을 이용하여 모델의 예측 값과 실제 값 사이의 오차를 최소화하는 방향으로 매개변수를 조정하는 것을 목표로 합니다.\n",
    "\n",
    "신경망 학습 단계:\n",
    "\n",
    "1. 데이터 준비: 학습에 사용할 입력 데이터와 해당 데이터의 정답(타깃)을 준비합니다. 일반적으로 데이터는 훈련 데이터와 검증 데이터, 그리고 시험 데이터로 나뉩니다.\n",
    "\n",
    "2. 초기화: 신경망의 가중치와 편향을 무작위로 초기화합니다. 이 초기화는 모델의 성능과 학습 과정에 영향을 미칩니다.\n",
    "\n",
    "3. 순전파 (Forward Propagation): 초기화된 신경망을 사용하여 입력 데이터를 순방향으로 전달하여 예측 값을 계산합니다.\n",
    "\n",
    "4. 손실 함수 계산: 순전파를 통해 얻은 예측 값과 실제 타깃 값 사이의 오차를 계산하기 위해 손실 함수를 사용합니다. 일반적으로 평균 제곱 오차(Mean Squared Error) 또는 교차 엔트로피 오차(Cross-Entropy Error)와 같은 손실 함수를 사용합니다.\n",
    "\n",
    "5. 역전파 (Backpropagation): 손실 함수의 값을 최소화하기 위해 역전파 알고리즘을 사용하여 오차를 역방향으로 전파하며, 가중치와 편향을 조정합니다.\n",
    "\n",
    "6. 매개변수 업데이트: 역전파를 통해 계산된 그래디언트를 사용하여 최적화 알고리즘(예: 경사 하강법)을 이용하여 가중치와 편향을 업데이트합니다.\n",
    "\n",
    "7. 3~6단계를 반복: 훈련 데이터를 사용하여 여러 에포크(epoch)에 걸쳐 반복적으로 모델을 학습시킵니다.\n",
    "\n",
    "8. 검증 데이터를 사용하여 모델 성능 평가: 학습 과정에서 사용하지 않은 검증 데이터를 사용하여 모델의 성능을 평가하고, 과적합(Overfitting)을 방지하기 위해 조정합니다.\n",
    "\n",
    "9. 시험 데이터를 사용하여 최종 성능 평가: 학습된 모델을 시험 데이터를 사용하여 최종적으로 평가합니다.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "*\"학습: 훈련데이터로부터 가중치 매개변수의 최적값을 자동으로 찾는 것\"*\n",
    "\n",
    "*\"신경망 학습의 목표: 손실함수의 결괏값을 가장 작게만드는 가중치 매개변수를 찾는 것\"*\n",
    "\n",
    "\n",
    "<img src=\"../Neural Network Training/assets/img/Screenshot 2023-07-28 at 4.41.51 PM.png\" style=\"width: 700px; height: 250px;\">\n",
    "\n",
    "**\"컴퓨터 비전 분야에서 SIFT,SURF,HOG라는 알고리즘은 이미지의 주요 특징을 추출해, 특징 벡터로 변환하고, 변환된 벡터를 가지고 지도학습 방식의 분류기법인 SVM,KNN등으로 학습 가능\"**\n",
    "\n",
    "> ###### *SIFT(Scale-Invariant Feature Transform): 이미지의 크기와 회전에 불변적인 특징점을 검출하고 특징을 디스크립터(이미지나 객체의 특징을 수치적으로 표현하는 알고리즘)로 표현하는 알고리즘\n",
    "> ###### *SURF(Speeded-Up Robust Features): SIFT와 유사한 크기와 회전에 불변적인 특징점을 빠르게 검출하는 알고리즘\n",
    "> ###### *HOG(Histogram of Oriented Gradients): 이미지의 그래디언트 방향 정보를 히스토그램(데이터의 빈도수를 막대 그래프로 시각화한 것으로, 값의 분포를 나타냄)으로 표현하여 객체의 형태를 나타내는 특징을 추출하는 알고리즘.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### **접근 방식의 문제점: 인간의 개입**\n",
    "\n",
    "<img src=\"../Neural Network Training/assets/img/Screenshot 2023-07-28 at 5.05.23 PM.png\" style=\"width: 700px; height: 350px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **훈련 데이터와 시험데이터**\n",
    "- *기계학습 문제는 훈련데이터와 시험데이터로 나눠 학습과 실험을 수행*  \n",
    "- *훈련데이터만을 사용해서 최적의 매개변수를 찾고,시험데이터를 사용하여 모델의 실력을 평가*\"\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "##### **기계학습 순서**\n",
    "*훈련 데이터를 학습하며 최적의 매개변수를 찾음 > 시험 데이터를 사용하여 앞서 훈련된 모델의 실력을 평가*\n",
    "\n",
    "*훈련 데이터와 시험 데이터를 나누는 이유 : **범용적으로 사용하기 위함**\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### **손실함수**\n",
    "*\"신경망 학습에서 사용되는 하나의 지표로, 최적의 매개변수 값을 탐색하기위해 평균 제곱 오차(MSE), 교차엔트로피오차(CEE)를 사용\"*\n",
    "\n",
    "*\"현재 신경망이 훈련 데이터를 얼마나 잘 처리하지 못하는지를 나타냄\"*\n",
    "\n",
    "#### **평균제곱오차(MSE)**\n",
    "<img src=\"../Neural Network Training/assets/img/Screenshot 2023-07-28 at 6.00.31 PM.png\" style=\"width: 500px; height: 150px;\">\n",
    "\n",
    "${k}$ : 데이터 차원 수,\n",
    "${y_k}$ : 신경망의 출력,\n",
    "${t_k}$ : 정답 레이블\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **평균제곱오차(MSE)구현하기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mean_squared_error(y, t):\n",
    "    return 0.5 * np.sum((y-t)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*\"함수 내부에서는 먼저 y와 t의 차이를 제곱한 값 (y-t)^2를 계산하고, np.sum() 함수를 사용하여 이 제곱 오차들의 합을 구합니다. 마지막으로 0.5를 곱하여 평균제곱오차를 계산합니다.\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09750000000000003"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0] # 정답 '2'\n",
    "\n",
    "# 예1 : '2'일 확률이 높다고 추정(0.6)\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0] # 소프트맥스 함수의 출력 \n",
    "mean_squared_error(np.array(y), np.array(t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5975"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예2: '7'일 확률이 가장 높다고 추정(0.06)\n",
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0] # 소프트맥스 함수의 출력 \n",
    "mean_squared_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **교차 엔트로피 오차(CEE)**\n",
    "\n",
    "*\"교차 엔트로피 오차(Cross-Entropy Error)는 주로 분류 문제에서 사용되는 손실 함수 중 하나이고, 모델의 예측과 실제 타깃(레이블) 값 사이의 차이를 측정하여 모델의 성능을 평가하고 최적화하는 데 활용\"*\n",
    "\n",
    "<img src=\"../Neural Network Training/assets/img/Screenshot 2023-07-29 at 11.23.46 AM.png\" style=\"width: 500px; height: 120px;\">\n",
    "\n",
    "##### **수식표현의 의미**\n",
    "*​${t_k}$ : 실제값(타겟)으로, 정답 레이블을 나타내는 배열입니다.*\n",
    "\n",
    "*${t_k}$ 배열은 모든 클래스에 대해 0 또는 1의 값을 가지며, 정답 클래스에 해당하는 원소만 1이고 나머지 원소는 0입니다. 이것은 정답 레이블을 원-핫 인코딩 형태로 표현한 것입니다.*\n",
    "\n",
    "*${y_k}$ : 모델의 예측값으로, 모델이 각 클래스에 대해 출력하는 확률 값을 나타내는 배열입니다. 모델의 출력값은 보통 확률로 표현되며, 각 클래스에 대해 0과 1 사이의 값으로 표현됩니다.*\n",
    "\n",
    "*$log({y_k})$: ${y_k}$ 배열의 각 원소에 대해 자연로그를 취합니다. 로그를 취하는 이유는 모델의 예측값과 실제값 간의 오차를 측정하는 지표로서 교차엔트로피를 사용하기 위함입니다.*\n",
    "\n",
    "*${t_k}log({y_k})$: 실제값과 예측값 간의 원소별 곱을 수행합니다. 이것은 오차를 표현하는데 사용됩니다.*\n",
    " \n",
    "*${t_k}$ 배열은 정답 레이블이므로 대응하는 ${y_k}$ 값이 1에 가까울수록 오차가 작아집니다.*\n",
    "\n",
    "*$\\sum_{k} t_k \\log(y_k)$: 모든 클래스에 대해 $t_k \\log(y_k)$ 값을 합산합니다. 이것은 모델의 전체 예측과 실제값 간의 오차를 나타냅니다.*\n",
    "\n",
    "*$-\\sum_{k} t_k \\log(y_k)$: 마지막으로 오차를 음수로 변환하여 교차엔트로피오차를 계산합니다. 음수를 붙이는 이유는 교차엔트로피를 최소화하는 방향으로 모델을 학습하기 위해서입니다.최적화 알고리즘은 일반적으로 손실 함수를 최소화하는 방향으로 모델을 학습하므로, 교차엔트로피오차에 음수를 붙여서 손실 함수를 최소화하는 방향으로 학습하게 됩니다.*\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"../Neural Network Training/assets/img/Screenshot 2023-07-29 at 11.33.07 AM.png\" style=\"width: 500px; height: 400px;\">\n",
    "\n",
    "- 확률 곱셈의 덧셈 변환: 자연로그를 취하면 확률 곱셈이 덧셈으로 변환되어 계산이 간편해짐\n",
    "\n",
    "- 수치 안정성: 자연로그를 사용하면 수치 안정성이 향상, 특히 매우 작은 확률 값을 다룰 때에도 로그 변환을 통해 수치적으로 안정적인 계산이 가능\n",
    "\n",
    "- 손실 함수의 미분: 자연로그를 취한 교차 엔트로피 오차의 미분이 간단하고 편리,이를 통해 경사 하강법과 같은 최적화 알고리즘에서 매개변수를 업데이트하는데 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **크로스 엔트로피 오차를 계산하는 파이썬 함수**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y,t):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t*np.log(y+delta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*np.log()에 0을 입력하면 -$\\infty$ 가 되어 계산을 진행할 수 없기 때문에, delta를 더해 방지*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.510825457099338"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "cross_entropy_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.302584092994546"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "cross_entropy_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### **미니배치 학습**\n",
    "\n",
    "*\"미니배치: 데이터를 작은 묶음으로 나누어 처리하는 방식\"*\n",
    "\n",
    "*\"주어진 데이터셋이 빅데이터수준이라면 교차 엔트로피 오차로 손실 함수의 합을 구하려면 많은 시간이 소요되는 문제가 있기 때문에, 데이터 일부를 무작위로 추려 전체의 근사치로 이용하는데, 여기서 그 일부를 미니배치라고 하고, 이러한 학습 방법을 미니배치 학습이라고 함*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "# 미니 배치학습 구현\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = \\\n",
    "    load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(t_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 무작위 추출\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 10\n",
    "batch_mask = np.random.choice(train_size, batch_size)\n",
    "x_batch = x_train[batch_mask]\n",
    "t_batch = t_train[batch_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([32351,  5767, 43482, 31090, 15269, 40161, 13202, 47391, 20502,\n",
       "       46318])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(60000, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(배치용)교차 엔트로피 오차 구현하기**\n",
    "*\"배치: 전체 데이터셋으로 한 번에 처리하는 방식\"*\n",
    "\n",
    "\n",
    "배치용 교차 엔트로피 오차를 구현하는 이유: 미니배치(mini-batch) 학습을 위해 전체 데이터셋을 작은 묶음으로 나누어 처리하는 경우에 효율적인 계산을 가능하게 하기 위함\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**장점**\n",
    "\n",
    "- 학습 속도 개선: 전체 데이터셋을 한 번에 처리하는 것보다 미니배치로 처리하면 학습 속도가 향상\n",
    "\n",
    "- 메모리 효율성: 전체 데이터를 한 번에 메모리에 올리는 것보다 미니배치로 처리하면 메모리 효율성이 높아지기 때문\n",
    "\n",
    "- 일반화 성능 향상: 미니배치를 사용하면 데이터셋이 무작위로 섞이기 때문에 모델이 데이터의 순서에 영향을 받지 않게 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1,y.size)\n",
    "\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t * np.log(y + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **왜 손실 함수를 설정하는가**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- **모델의 성능 평가**: 손실 함수는 모델이 얼마나 정확하게 예측하는지를 측정하는 지표로 사용\n",
    "\n",
    "- **최적화를 위한 목표 제공**: 최적화 알고리즘을 사용하여 손실 함수를 최소화하는 방향으로 모델의 가중치와 편향을 업데이트 함. 이때, 손실 함수가 최소가 되는 지점은 모델의 예측이 실제 정답과 가장 가까운 지점이므로, 손실 함수는 모델을 학습하는 데에 목표를 제공\n",
    "\n",
    "- **다중 클래스 분류 문제에서 확률 분포 표현**: 손실 함수는 다중 클래스 분류 문제에서 모델의 출력을 확률 분포로 표현하는 데에 사용됨, 이를 통해 모델이 각 클래스에 속할 확률을 예측하고, 정답과의 차이를 계산\n",
    "\n",
    "*\"신경망 학습에서는 최적의 매개변수(가중치와 편향)를 탐색할 때 손실 함수의 값을 가능한 한 작게 하는 매개변수 값을 찾는데, 이때 매개변수의 미분(기울기)을 계산하고, 그 미분 값을 단서로 매개변수의 값을 갱신하는 과정을 반복\"*\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "##### **정확도를 지표로 삼으면?**\n",
    "*\"신경망을 학습할 때 정확도를 지표로 하면 매개변수의 미분이 대부분의 장소에서 0이 되기 때문에 정확도를 지표로 삼아서는 안됨\"*  \n",
    "*\"계단함수를 활성화 함수로 사용하지 않듯, 정확도는 매개변수의 미소한 변화에 거의 반응을 보이지 않기 때문에, 반응이 있더라도 그 값이 불연속적으로 갑자기 변화함\"*\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## **수치 미분**\n",
    "\n",
    "\n",
    "<img src=\"../Neural Network Training/assets/img/Screenshot 2023-07-31 at 8.10.19 AM.png\" style=\"width: 500px; height: 100px;\">\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"../Neural Network Training/assets/img/Screenshot 2023-07-31 at 8.29.50 AM.png\" style=\"width: 500px; height: 350px;\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_diff(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    return(f(x+h) - f(x-h)) / (2*h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **편미분**\n",
    "\n",
    "<img src=\"../Neural Network Training/assets/img/Screenshot 2023-07-31 at 8.38.19 AM.png\" style=\"width: 500px; height: 100px;\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 편미분 구현\n",
    "def function_2(x):\n",
    "    return x[0]**2 + x[1**2]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"../Neural Network Training/assets/img/Screenshot 2023-07-31 at 8.40.12 AM.png\" style=\"width: 500px; height: 300px;\">\n",
    "\n",
    "## **기울기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "\n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "\n",
    "        # f(x + h) 계산\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "\n",
    "        # f(x - h) 계산\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "\n",
    "        # 중앙 차분으로 수치 미분\n",
    "        grad[idx] = (fxh1 - fxh2) / (2 * h)\n",
    "        x[idx] = tmp_val\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6., 8.])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_gradient(function_2, np.array([3.0, 4.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **경사법(경사하강법)**\n",
    "\n",
    "*\"경사법(Gradient Descent)은 기계 학습과 최적화 알고리즘에서 널리 사용되는 최적화 방법 중 하나입니다. 이 방법은 함수의 최솟값(또는 최댓값)을 찾는데 사용됩니다. 특히 머신 러닝에서는 손실 함수(loss function)의 최솟값을 찾기 위해 주로 적용됩니다.\"*\n",
    "\n",
    "*\"경사법은 이름 그대로 함수의 기울기(경사)를 이용하여 최솟값을 찾는 방법입니다. 함수의 기울기는 해당 지점에서 가장 빠르게 증가하는 방향을 가리키며, 최솟값을 찾기 위해서는 기울기가 가리키는 방향의 반대 방향으로 이동하는 것이 핵심 원리입니다.\"*\n",
    "\n",
    "#### **경사법의 작동 방식**\n",
    "\n",
    "1. 초기값을 설정합니다. 이 값은 함수의 최솟값을 찾기 위한 시작점입니다.\n",
    "\n",
    "2. 초기값에서 함수의 기울기(경사)를 계산합니다. 이를 통해 현재 위치에서 함수의 증가 방향을 알 수 있습니다.\n",
    "\n",
    "3. 기울기의 반대 방향으로 조금씩 이동합니다. 이동 거리는 학습률(learning rate)이라고 불리는 하이퍼파라미터로 조절됩니다. 작은 학습률은 안정적이지만 수렴하는 데 오래 걸릴 수 있고, 큰 학습률은 빠르게 수렴하지만 발산할 수 있습니다.\n",
    "\n",
    "4. 새로운 위치에서 다시 기울기를 계산하고, 2번과 3번의 단계를 반복합니다.\n",
    "\n",
    "5. 함수의 기울기가 거의 0에 가까워질 때까지 이동하며, 이 때의 위치가 최솟값으로 추정됩니다.\n",
    "\n",
    "6. 경사법은 다양한 종류가 있으며, 가장 기본적인 형태는 배치 경사 하강법(Batch Gradient Descent)입니다. 또한, 미니 배치 경사 하강법(Mini-batch Gradient Descent)와 확률적 경사 하강법(Stochastic Gradient Descent)과 같은 변형들도 있습니다.\n",
    "\n",
    "##### **수식으로 나타낸 경사법**\n",
    "\n",
    "\n",
    "<img src=\"../Neural Network Training/assets/img/Screenshot 2023-07-31 at 9.01.33 AM.png\" style=\"width: 500px; height: 170px;\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코드로 구현한 경사법\n",
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "\n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*f: 최적화 하려는 함수  \n",
    "*init_x는 초깃값  \n",
    "*lr: learning rate  \n",
    "*step_num: 경사법에 따른 반복횟수  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **하이퍼파라미터란?**\n",
    "*\"신경망의 가중치 매개변수는 훈련 데이터와 학습 알고리즘에 의해서 자동으로 획득되는 매개변수인 반면, 학습률 같은 하이퍼파라미터는 사람이 직접 설정해야 하는 매개변수\"*\n",
    "\n",
    "## **신경망에서의 기울기**\n",
    "\n",
    "<img src=\"../Neural Network Training/assets/img/Screenshot 2023-07-31 at 9.37.40 AM.png\" style=\"width: 500px; height: 170px;\">\n",
    "\n",
    "\n",
    "## **학습 알고리즘 구현**\n",
    "<img src=\"../Neural Network Training/assets/img/Screenshot 2023-07-31 at 9.45.06 AM.png\" style=\"width: 600px; height: 300px;\">\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"../Neural Network Training/assets/img/Screenshot 2023-07-31 at 10.03.09 AM.png\" style=\"width: 700px; height: 600px;\">\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"../Neural Network Training/assets/img/Screenshot 2023-07-31 at 10.14.29 AM.png\" style=\"width: 600px; height: 300px;\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
